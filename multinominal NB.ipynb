{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### imports for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tommy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tommy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import random\n",
    "from prettytable import PrettyTable\n",
    "import textwrap \n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import excel with pandas Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_emotions = pd.read_excel('Diabetes-classification.xlsx', sheet_name ='Emotions')\n",
    "\n",
    "# Preparing dataset\n",
    "x_emotion = df_emotions.loc[:,'discussion_text']\n",
    "y_emotion = df_emotions.loc[:,'Label']\n",
    "\n",
    "# removes all duplicates from list \n",
    "Labels_emotion = list(dict.fromkeys(y_emotion))\n",
    "\n",
    "#Remove stopwords\n",
    "lim_punc = [char for char in string.punctuation if char in \"&#^_\"]\n",
    "nopunc = [char for char in x_emotion if char not in lim_punc]\n",
    "nopunc = ''.join(nopunc)\n",
    "\n",
    "other_stop=['•','...in','...the','...you\\'ve','–','—','-','⋆','...','....','..','C.','c','|','...The','...The','...When','...A','C','+','1','2','3','4','5','6','7','8','9','10', '2016',  'speak','also', 'seen','[5].',  'using', 'get',  'instead',  \"that's\",  '......','may', 'e', '...it', 'puts', '...over', '[✯]','happens', \"they're\",'hwo',  '...a', 'called',  '50s','c;', '20',  'per', 'however,','it,', 'yet', 'one', 'bs,', 'ms,', 'sr.',  '...taking',  'may', '...of', 'course,', 'get', 'likely', 'no,']\n",
    "\n",
    "ext_stopwords=stopwords.words('english')+other_stop\n",
    "clean_words = [word for word in nopunc.split() if word.lower() not in ext_stopwords]\n",
    "# puts discussion_text to a str and tokenize it\n",
    "#raw_text_emotion = df_emotions['discussion_text'].str.cat()\n",
    "raw_text_emotion = df_emotions['discussion_text'].str.cat()\n",
    "\n",
    "tokens_emotion = nltk.word_tokenize(raw_text_emotion)\n",
    "tokens_emotion_filtered = [clean_words for clean_words in tokens_emotion if clean_words]\n",
    "text_emotion = nltk.Text(tokens_emotion_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinominal NB classifer for Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sadness\n",
      "Joy\n",
      "+------------------------------------------+---------+\n",
      "|             Discussion text              | Emotion |\n",
      "+------------------------------------------+---------+\n",
      "| Am,freaking,out,that,am,going,to,lose,my | Sadness |\n",
      "| ,sight?.,What,does,the,pressure,mean?.,I |         |\n",
      "| ,have,only,had,diabetes,2,years,and,am,g |         |\n",
      "| etting,depressed,that,this,is,the,start, |         |\n",
      "|         of,things,to,come:(:(:(          |         |\n",
      "|                                          |         |\n",
      "|                                          |         |\n",
      "| I,was,diagnosed,with,PCOS,about,10,years |   Joy   |\n",
      "| ,ago..,At,that,time,glucose,tests,showed |         |\n",
      "| ,no,problems,with,how,my,body,processed, |         |\n",
      "| insulin..,I,have,since,progressed,to,Typ |         |\n",
      "| e,2,diabetes..,The,only,way,I,have,ever, |         |\n",
      "| lost,weight,in,my,entire,life,is,by,watc |         |\n",
      "|          hing,my,carbs,,however          |         |\n",
      "|                                          |         |\n",
      "|                                          |         |\n",
      "+------------------------------------------+---------+\n"
     ]
    }
   ],
   "source": [
    "# the reviews will be stored as document pairs of words and category\n",
    "X_list_of_words = [sentence.split(\" \") for sentence in x_emotion]\n",
    "documents = list(zip(X_list_of_words, y_emotion))\n",
    "\n",
    "#give random order to the documents\n",
    "random.shuffle(documents)\n",
    "\n",
    "tab = PrettyTable(['Discussion text', 'Emotion'])\n",
    "tab.horizontal_char = '-'\n",
    "\n",
    "for (doc, cat) in documents[0:2]:\n",
    "    feats = textwrap.fill(','.join(doc[:50]), width=40)\n",
    "    tab.add_row([ feats, cat])\n",
    "    tab.add_row([ '\\n', '\\n'])\n",
    "    print(cat)\n",
    "\n",
    "print(tab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words from emotion corpus:  276377\n",
      "most freq words:  [('think', 398), ('other', 398), ('does', 393), ('many', 389), ('only', 387), ('day', 385), ('time', 383), ('much', 380), ('help', 379), ('risk', 376)]\n",
      "word_features[:25]:  ['i', '..', 'and', 'the', 'to', 'a', 'of', 'diabetes', '2', 'type', 'is', 'my', 'that', 'have', 'in', 'it', 'with', 'for', 'you', 'was', 'on', 'as', 'not', 'but', ')']\n"
     ]
    }
   ],
   "source": [
    "print('total words from emotion corpus: ', len(text_emotion))\n",
    "\n",
    "# load all the words in freq distribution\n",
    "all_words = nltk.FreqDist(w.lower() for w in text_emotion)\n",
    "\n",
    "#construct a list of the 2000 most frequent words in the overall corpus (you can try with other numbers as well)\n",
    "most_freq_words = all_words.most_common(6000)\n",
    "print('most freq words: ', most_freq_words[100:110])\n",
    "\n",
    "word_features = [word for (word, count) in most_freq_words]\n",
    "print('word_features[:25]: ', word_features[:25])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformed document features, printing the first 25 features \n",
      "\n",
      " {'contains(i)': True, 'contains(..)': True, 'contains(and)': True, 'contains(the)': True, 'contains(to)': True, 'contains(a)': True, 'contains(of)': True, 'contains(diabetes)': True, 'contains(2)': True, 'contains(type)': True, 'contains(is)': True, 'contains(my)': True, 'contains(that)': True, 'contains(have)': True, 'contains(in)': True, 'contains(it)': True, 'contains(with)': True, 'contains(for)': True, 'contains(you)': True, 'contains(was)': True, 'contains(on)': True, 'contains(as)': True, 'contains(not)': True, 'contains(but)': True, 'contains())': True}\n"
     ]
    }
   ],
   "source": [
    "def get_document_features(document, doc_features):\n",
    "    \"\"\"\n",
    "        This function will convert given document into a feature set.\n",
    "        Note that we need to add the feature set that is relevant to the document we are inputting\n",
    "        \n",
    "    \"\"\"\n",
    "    #checking whether a word occurs in a set is much faster than checking whether it occurs in a list \n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    \n",
    "    #the feaures dict will consist of words as keys and boolean value of whether they exist in the document\n",
    "    for word in doc_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "\n",
    "# test code for the above function\n",
    "#words_doc = movie_reviews.words('pos/cv957_8737.txt')\n",
    "words_doc = text_emotion\n",
    "\n",
    "feat_dict = get_document_features(words_doc, word_features)\n",
    "\n",
    "feat_dict_25 = {k: feat_dict[k] for k in list(feat_dict.keys())[:25]}\n",
    "print('transformed document features, printing the first 25 features \\n\\n', feat_dict_25)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44666666666666666\n",
      "Most Informative Features\n",
      "         contains(bacon) = True            Anger : Trust  =     21.6 : 1.0\n",
      "          contains(hold) = True            Anger : Trust  =     21.6 : 1.0\n",
      "         contains(local) = True            Anger : Trust  =     21.6 : 1.0\n",
      "    contains(production) = True            Anger : Trust  =     21.6 : 1.0\n",
      "      contains(reaction) = True            Anger : Trust  =     21.6 : 1.0\n",
      "         contains(shift) = True            Anger : Trust  =     21.6 : 1.0\n",
      "          contains(bike) = True             Fear : Trust  =     19.8 : 1.0\n",
      "          contains(seek) = True           Surpri : Trust  =     19.2 : 1.0\n",
      "        contains(action) = True            Anger : Trust  =     18.1 : 1.0\n",
      "       contains(sitting) = True            Anger : Trust  =     18.1 : 1.0\n",
      "        contains(spikes) = True            Anger : Trust  =     18.1 : 1.0\n",
      "           contains(2nd) = True            Anger : Antici =     17.8 : 1.0\n",
      "         contains(story) = True            Anger : Antici =     17.8 : 1.0\n",
      "            contains(ie) = True             Fear : Antici =     16.4 : 1.0\n",
      "          contains(okay) = True           Surpri : Antici =     15.9 : 1.0\n",
      "           contains(non) = True           Disgus : Antici =     14.5 : 1.0\n",
      "         contains(basal) = True             Fear : Trust  =     14.2 : 1.0\n",
      "  contains(improvements) = True             Fear : Trust  =     14.2 : 1.0\n",
      "       contains(loosing) = True             Fear : Trust  =     14.2 : 1.0\n",
      "  contains(osteoporosis) = True             Fear : Trust  =     14.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "#obtain feature set\n",
    "featuresets = [(get_document_features(d,word_features), c) for (d,c) in documents]\n",
    "\n",
    "#split into train and test set (you can experiment with distribution here) 100 - 100 og\n",
    "train_set, test_set = featuresets[100:2500], featuresets[:300]\n",
    "\n",
    "#instantiate classifier\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "#print accuracy and most informative features\n",
    "print(nltk.classify.accuracy(classifier, test_set)) \n",
    "\n",
    "classifier.show_most_informative_features(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             |     A                   |\n",
      "             |     n                   |\n",
      "             |     t                   |\n",
      "             |     i                   |\n",
      "             |     c              S    |\n",
      "             |     i  D        S  u    |\n",
      "             |     p  i        a  r    |\n",
      "             |  A  a  s        d  p  T |\n",
      "             |  n  t  g  F     n  r  r |\n",
      "             |  g  i  u  e  J  e  i  u |\n",
      "             |  e  o  s  a  o  s  s  s |\n",
      "             |  r  n  t  r  y  s  e  t |\n",
      "-------------+-------------------------+\n",
      "       Anger | <1> 4  .  .  .  .  .  1 |\n",
      "Anticipation |  .<35> 2  .  8  5  . 33 |\n",
      "     Disgust |  .  4<10> .  1  1  . 15 |\n",
      "        Fear |  .  3  . <1> 2  .  1  3 |\n",
      "         Joy |  .  2  2  . <6> 1  .  7 |\n",
      "     Sadness |  1  2  1  .  . <8> . 15 |\n",
      "    Surprise |  .  2  .  .  .  . <2>18 |\n",
      "       Trust |  1 14  7  1  3  6  .<71>|\n",
      "-------------+-------------------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "refsets = defaultdict(set)\n",
    "testsets = defaultdict(set)\n",
    "labels = []\n",
    "tests = []\n",
    "for i, (feats, label) in enumerate(test_set):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    labels.append(label)\n",
    "    tests.append(observed)\n",
    "\n",
    "print(nltk.ConfusionMatrix(labels, tests))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result of sample review:  Joy\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sample_review = \"My sickness got worse, and the doctors won't do anything\"\n",
    "\n",
    "#get features specific to the input text\n",
    "sample_features = {word:True for word in sample_review.split()}\n",
    "\n",
    "\n",
    "sample_review_doc_feats = get_document_features(sample_review.split(),sample_features)\n",
    "\n",
    "\n",
    "#print('Sample review features: \\n\\n',sample_review_doc_feats)\n",
    "\n",
    "print('result of sample review: ', classifier.classify(sample_review_doc_feats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loads in Patient Journey labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_patient = pd.read_excel('Diabetes-classification.xlsx', sheet_name='Patient-journey') # Reads in excel\n",
    "\n",
    "# Preparing dataset\n",
    "x_journey = df_patient.loc[:,'discussion_text']\n",
    "y_journey = df_patient.loc[:,'Label']\n",
    "# removes all duplicates from list \n",
    "Labels_journey = list(dict.fromkeys(y_journey)) \n",
    "#stopwords\n",
    "lim_punc_patient = [char for char in string.punctuation if char in \"&#^_\"]\n",
    "nopunc_patient = [char for char in x_journey if char not in lim_punc_patient]\n",
    "nopunc_patient = ''.join(nopunc_patient)\n",
    "\n",
    "ext_stopwords_patient=stopwords.words('english')+other_stop\n",
    "clean_words = [word for word in nopunc_patient.split() if word.lower() not in ext_stopwords_patient]\n",
    "\n",
    "# puts discussion_text to a str and tokenize it\n",
    "raw_text_journey = df_patient['discussion_text'].str.cat()\n",
    "tokens_journey = nltk.word_tokenize(raw_text_journey)\n",
    "tokens_emotion_filtered = [clean_words for clean_words in tokens_journey if clean_words.isalnum()]\n",
    "text_journey = nltk.Text(tokens_journey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagnosis \n",
      "Living with diabetes - Exercise\n",
      "+------------------------------------------+---------------------------------+\n",
      "|             Discussion text              |             Emotion             |\n",
      "+------------------------------------------+---------------------------------+\n",
      "| 0,Going,through,the,MDs,door,for,a,routi |            Diagnosis            |\n",
      "| ne,check-up,,coming,out,with,a,type,2,di |                                 |\n",
      "| agnosis..,Wow!.,How,in,the,world,did,thi |                                 |\n",
      "|                s,happen?                 |                                 |\n",
      "|                                          |                                 |\n",
      "|                                          |                                 |\n",
      "| Can,you,cure,it?.,If,you,are,not,genetic | Living with diabetes - Exercise |\n",
      "| ally,predisposed,and,aren't,over,50--60% |                                 |\n",
      "| ,of,newly,diagnosed,type,2,diabetics,are |                                 |\n",
      "| ,over,50,as,age,naturally,slows,down,the |                                 |\n",
      "| ,metabolism--then,there,is,no,reason,to, |                                 |\n",
      "| believe,you,can't,slow,or,halt,the,progr |                                 |\n",
      "|    ession,by,taking,steps,now,,but,if    |                                 |\n",
      "|                                          |                                 |\n",
      "|                                          |                                 |\n",
      "+------------------------------------------+---------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# the reviews will be stored as document pairs of words and category\n",
    "X_list_of_words_journey = [sentence.split(\" \") for sentence in x_journey]\n",
    "documents_journey = list(zip(X_list_of_words_journey, y_journey))\n",
    "\n",
    "#give random order to the documents\n",
    "random.shuffle(documents_journey)\n",
    "\n",
    "tab = PrettyTable(['Discussion text', 'Emotion'])\n",
    "tab.horizontal_char = '-'\n",
    "\n",
    "for (doc, cat) in documents_journey[0:2]:\n",
    "    feats_journey = textwrap.fill(','.join(doc[:50]), width=40)\n",
    "    tab.add_row([ feats_journey, cat])\n",
    "    tab.add_row([ '\\n', '\\n'])\n",
    "    print(cat)\n",
    "\n",
    "print(tab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words from emotion corpus:  116721\n",
      "most freq words:  [('his', 165), ('exercise', 165), ('only', 159), ('she', 159), ('time', 159), ('well', 157), ('any', 157), ('glucose', 156), ('then', 155), ('disease', 154)]\n",
      "word_features[:25]:  ['..', 'i', 'and', 'the', 'to', 'a', 'of', 'diabetes', '2', 'type', 'is', 'in', 'my', 'that', 'with', 'have', 'for', 'it', 'you', 'was', 'on', 'as', 'are', ')', '.']\n"
     ]
    }
   ],
   "source": [
    "print('total words from emotion corpus: ', len(text_journey))\n",
    "\n",
    "# load all the words in freq distribution\n",
    "all_words_journey = nltk.FreqDist(w.lower() for w in text_journey)\n",
    "\n",
    "#construct a list of the 2000 most frequent words in the overall corpus (you can try with other numbers as well)\n",
    "most_freq_words_journey = all_words_journey.most_common(6000)\n",
    "print('most freq words: ', most_freq_words_journey[100:110])\n",
    "\n",
    "word_features_journey = [word for (word, count) in most_freq_words_journey]\n",
    "print('word_features[:25]: ', word_features_journey[:25])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformed document features, printing the first 25 features \n",
      "\n",
      " {'contains(..)': True, 'contains(i)': True, 'contains(and)': True, 'contains(the)': True, 'contains(to)': True, 'contains(a)': True, 'contains(of)': True, 'contains(diabetes)': True, 'contains(2)': True, 'contains(type)': True, 'contains(is)': True, 'contains(in)': True, 'contains(my)': True, 'contains(that)': True, 'contains(with)': True, 'contains(have)': True, 'contains(for)': True, 'contains(it)': True, 'contains(you)': True, 'contains(was)': True, 'contains(on)': True, 'contains(as)': True, 'contains(are)': True, 'contains())': True, 'contains(.)': True}\n"
     ]
    }
   ],
   "source": [
    "def get_document_features_journey(documents_journey, doc_features):\n",
    "    \"\"\"\n",
    "        This function will convert given document into a feature set.\n",
    "        Note that we need to add the feature set that is relevant to the document we are inputting\n",
    "        \n",
    "    \"\"\"\n",
    "    #checking whether a word occurs in a set is much faster than checking whether it occurs in a list \n",
    "    document_words = set(documents_journey)\n",
    "    features = {}\n",
    "    \n",
    "    #the feaures dict will consist of words as keys and boolean value of whether they exist in the document\n",
    "    for word in doc_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "words_doc = text_journey\n",
    "\n",
    "feat_dict = get_document_features(words_doc, word_features_journey)\n",
    "\n",
    "feat_dict_25 = {k: feat_dict[k] for k in list(feat_dict.keys())[:25]}\n",
    "print('transformed document features, printing the first 25 features \\n\\n', feat_dict_25)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n",
      "Most Informative Features\n",
      "    contains(colleagues) = True           Altern : Living =     61.0 : 1.0\n",
      "      contains(exercise) = True           Living : Living =     45.0 : 1.0\n",
      "       contains(prevent) = True           Altern : Living =     40.5 : 1.0\n",
      "    contains(treatments) = True           Altern : Living =     40.5 : 1.0\n",
      "            contains(35) = True           Altern : Living =     36.6 : 1.0\n",
      "        contains(acidic) = True           Altern : Living =     36.6 : 1.0\n",
      "    contains(additional) = True           Altern : Living =     36.6 : 1.0\n",
      "      contains(approved) = True           Altern : Living =     36.6 : 1.0\n",
      "        contains(bitter) = True           Altern : Living =     36.6 : 1.0\n",
      "         contains(bones) = True           Altern : Living =     36.6 : 1.0\n",
      "        contains(bought) = True           Altern : Living =     36.6 : 1.0\n",
      "          contains(drug) = True           Altern : Living =     36.6 : 1.0\n",
      "     contains(expensive) = True           Altern : Living =     36.6 : 1.0\n",
      "       contains(failing) = True           Altern : Living =     36.6 : 1.0\n",
      "    contains(flavonoids) = True           Altern : Living =     36.6 : 1.0\n",
      "        contains(future) = True           Altern : Living =     36.6 : 1.0\n",
      "        contains(ginger) = True           Altern : Living =     36.6 : 1.0\n",
      "      contains(guidance) = True           Altern : Living =     36.6 : 1.0\n",
      "          contains(heal) = True           Altern : Living =     36.6 : 1.0\n",
      "   contains(investigate) = True           Altern : Living =     36.6 : 1.0\n"
     ]
    }
   ],
   "source": [
    "#obtain feature sets for all movie reviews\n",
    "featuresets_journey = [(get_document_features_journey(d,word_features_journey), c) for (d,c) in documents_journey]\n",
    "\n",
    "#split into train and test set (you can experiment with distribution here) 100 - 100 og\n",
    "train_set_journey, test_set_journey = featuresets_journey[200:], featuresets_journey[:100]\n",
    "\n",
    "#instantiate classifier\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set_journey)\n",
    "\n",
    "#print accuracy and most informative features\n",
    "print(nltk.classify.accuracy(classifier, test_set_journey)) \n",
    "\n",
    "classifier.show_most_informative_features(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result of sample review:  Living with diabetes - Nutrition\n"
     ]
    }
   ],
   "source": [
    "sample_review = \"My doctor told me to start running and go on a diet\"\n",
    "\n",
    "#get features specific to the input text\n",
    "sample_features = {word:True for word in sample_review.split()}\n",
    "\n",
    "\n",
    "sample_review_doc_feats = get_document_features_journey(sample_review.split(),sample_features)\n",
    "\n",
    "\n",
    "#print('Sample review features: \\n\\n',sample_review_doc_feats)\n",
    "\n",
    "print('result of sample review: ', classifier.classify(sample_review_doc_feats))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a8f61be024eba58adef938c9aa1e29e02cb3dece83a5348b1a2dafd16a070453"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

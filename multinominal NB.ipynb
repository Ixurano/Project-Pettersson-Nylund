{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### imports for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tommy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tommy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import random\n",
    "from prettytable import PrettyTable\n",
    "import textwrap \n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "#display\n",
    "from IPython.display import display\n",
    "\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import excel with pandas Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_emotions = pd.read_excel('Diabetes-classification.xlsx', sheet_name ='Emotions')\n",
    "\n",
    "# Preparing dataset\n",
    "x_emotion = df_emotions.loc[:,'discussion_text']\n",
    "y_emotion = df_emotions.loc[:,'Label']\n",
    "\n",
    "# removes all duplicates from list \n",
    "Labels_emotion = list(dict.fromkeys(y_emotion))\n",
    "\n",
    "#Remove stopwords\n",
    "lim_punc = [char for char in string.punctuation if char in \"&#^_\"]\n",
    "nopunc = [char for char in x_emotion if char not in lim_punc]\n",
    "nopunc = ''.join(nopunc)\n",
    "\n",
    "other_stop=['•','...in','...the','...you\\'ve','–','—','-','⋆','...','....','..','C.','c','|','...The','...The','...When','...A','C','+','1','2','3','4','5','6','7','8','9','10', '2016',  'speak','also', 'seen','[5].',  'using', 'get',  'instead',  \"that's\",  '......','may', 'e', '...it', 'puts', '...over', '[✯]','happens', \"they're\",'hwo',  '...a', 'called',  '50s','c;', '20',  'per', 'however,','it,', 'yet', 'one', 'bs,', 'ms,', 'sr.',  '...taking',  'may', '...of', 'course,', 'get', 'likely', 'no,']\n",
    "\n",
    "ext_stopwords=stopwords.words('english')+other_stop\n",
    "clean_words = [word for word in nopunc.split() if word.lower() not in ext_stopwords]\n",
    "# puts discussion_text to a str and tokenize it\n",
    "#raw_text_emotion = df_emotions['discussion_text'].str.cat()\n",
    "raw_text_emotion = df_emotions['discussion_text'].str.cat()\n",
    "\n",
    "tokens_emotion = nltk.word_tokenize(raw_text_emotion)\n",
    "tokens_emotion_filtered = [clean_words for clean_words in tokens_emotion if clean_words]\n",
    "text_emotion = nltk.Text(tokens_emotion_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinominal NB classifer for Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anticipation\n",
      "Trust\n",
      "+------------------------------------------+--------------+\n",
      "|             Discussion text              |   Emotion    |\n",
      "+------------------------------------------+--------------+\n",
      "| Doctors,says,I,forced,my,diabetes,on,me, | Anticipation |\n",
      "| because,I,was,highly,stressed,out,after, |              |\n",
      "| my,brother,died..,Although,it's,genetic, |              |\n",
      "| I,didn't,have,it,till,then..,So,I,had,to |              |\n",
      "| ,deal,with,death,and,straight,after,diag |              |\n",
      "| nosed,diabetes,type,2..,I,can't,tell,fam |              |\n",
      "|        ily,because,like,I,said,my        |              |\n",
      "|                                          |              |\n",
      "|                                          |              |\n",
      "| I,don't,discuss,LC,with,her,often,,I,jus |    Trust     |\n",
      "| t,say,no,bread,,,sugar,,potatoes,etc,and |              |\n",
      "| ,smile..,She's,a,product,of,a,low,fat,wo |              |\n",
      "| rld,and,likely,in,her,youth,was,borderli |              |\n",
      "| ne,anorexic..,However,she,along,side,me, |              |\n",
      "| watched,mama,die,because,of,heart,issues |              |\n",
      "|                   ,and                   |              |\n",
      "|                                          |              |\n",
      "|                                          |              |\n",
      "+------------------------------------------+--------------+\n"
     ]
    }
   ],
   "source": [
    "# the reviews will be stored as document pairs of words and category\n",
    "X_list_of_words = [sentence.split(\" \") for sentence in x_emotion]\n",
    "documents = list(zip(X_list_of_words, y_emotion))\n",
    "\n",
    "#give random order to the documents\n",
    "random.shuffle(documents)\n",
    "\n",
    "tab = PrettyTable(['Discussion text', 'Emotion'])\n",
    "tab.horizontal_char = '-'\n",
    "\n",
    "for (doc, cat) in documents[0:2]:\n",
    "    feats = textwrap.fill(','.join(doc[:50]), width=40)\n",
    "    tab.add_row([ feats, cat])\n",
    "    tab.add_row([ '\\n', '\\n'])\n",
    "    print(cat)\n",
    "\n",
    "print(tab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words from emotion corpus:  276377\n",
      "most freq words:  [('think', 398), ('other', 398), ('does', 393), ('many', 389), ('only', 387), ('day', 385), ('time', 383), ('much', 380), ('help', 379), ('risk', 376)]\n",
      "word_features[:25]:  ['i', '..', 'and', 'the', 'to', 'a', 'of', 'diabetes', '2', 'type', 'is', 'my', 'that', 'have', 'in', 'it', 'with', 'for', 'you', 'was', 'on', 'as', 'not', 'but', ')']\n"
     ]
    }
   ],
   "source": [
    "print('total words from emotion corpus: ', len(text_emotion))\n",
    "\n",
    "# load all the words in freq distribution\n",
    "all_words = nltk.FreqDist(w.lower() for w in text_emotion)\n",
    "\n",
    "#construct a list of the 2000 most frequent words in the overall corpus (you can try with other numbers as well)\n",
    "most_freq_words = all_words.most_common(6000)\n",
    "print('most freq words: ', most_freq_words[100:110])\n",
    "\n",
    "word_features = [word for (word, count) in most_freq_words]\n",
    "print('word_features[:25]: ', word_features[:25])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformed document features, printing the first 25 features \n",
      "\n",
      " {'contains(i)': True, 'contains(..)': True, 'contains(and)': True, 'contains(the)': True, 'contains(to)': True, 'contains(a)': True, 'contains(of)': True, 'contains(diabetes)': True, 'contains(2)': True, 'contains(type)': True, 'contains(is)': True, 'contains(my)': True, 'contains(that)': True, 'contains(have)': True, 'contains(in)': True, 'contains(it)': True, 'contains(with)': True, 'contains(for)': True, 'contains(you)': True, 'contains(was)': True, 'contains(on)': True, 'contains(as)': True, 'contains(not)': True, 'contains(but)': True, 'contains())': True}\n"
     ]
    }
   ],
   "source": [
    "def get_document_features(document, doc_features):\n",
    "    \"\"\"\n",
    "        This function will convert given document into a feature set.\n",
    "        Note that we need to add the feature set that is relevant to the document we are inputting\n",
    "        \n",
    "    \"\"\"\n",
    "    #checking whether a word occurs in a set is much faster than checking whether it occurs in a list \n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    \n",
    "    #the feaures dict will consist of words as keys and boolean value of whether they exist in the document\n",
    "    for word in doc_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "\n",
    "# test code for the above function\n",
    "#words_doc = movie_reviews.words('pos/cv957_8737.txt')\n",
    "words_doc = text_emotion\n",
    "\n",
    "feat_dict = get_document_features(words_doc, word_features)\n",
    "\n",
    "feat_dict_25 = {k: feat_dict[k] for k in list(feat_dict.keys())[:25]}\n",
    "print('transformed document features, printing the first 25 features \\n\\n', feat_dict_25)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5166666666666667\n",
      "Most Informative Features\n",
      "    contains(production) = True             Fear : Trust  =     21.4 : 1.0\n",
      "         contains(bacon) = True            Anger : Trust  =     20.5 : 1.0\n",
      "    contains(incredibly) = True            Anger : Trust  =     20.5 : 1.0\n",
      "      contains(reaction) = True            Anger : Trust  =     20.5 : 1.0\n",
      "         contains(shift) = True            Anger : Trust  =     20.5 : 1.0\n",
      "        contains(action) = True            Anger : Trust  =     17.3 : 1.0\n",
      "         contains(eaten) = True            Anger : Trust  =     17.3 : 1.0\n",
      "        contains(stated) = True           Surpri : Trust  =     17.2 : 1.0\n",
      "         contains(lives) = True             Fear : Antici =     17.0 : 1.0\n",
      "           contains(odd) = True             Fear : Antici =     17.0 : 1.0\n",
      "      contains(specific) = True             Fear : Antici =     17.0 : 1.0\n",
      "    contains(motivation) = True            Anger : Antici =     16.3 : 1.0\n",
      "        contains(relate) = True            Anger : Antici =     16.3 : 1.0\n",
      "    contains(responding) = True            Anger : Antici =     16.3 : 1.0\n",
      "       contains(sitting) = True            Anger : Antici =     16.3 : 1.0\n",
      "         contains(story) = True            Anger : Antici =     16.3 : 1.0\n",
      "         contains(acids) = True             Fear : Trust  =     15.3 : 1.0\n",
      "         contains(basal) = True             Fear : Trust  =     15.3 : 1.0\n",
      "      contains(controls) = True             Fear : Trust  =     15.3 : 1.0\n",
      "          contains(lock) = True             Fear : Trust  =     15.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "#obtain feature set\n",
    "featuresets = [(get_document_features(d,word_features), c) for (d,c) in documents]\n",
    "\n",
    "#split into train and test set (you can experiment with distribution here) 100 - 100 og\n",
    "train_set, test_set = featuresets[100:2500], featuresets[:300]\n",
    "\n",
    "#instantiate classifier\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "#print accuracy and most informative features\n",
    "print(nltk.classify.accuracy(classifier, test_set)) \n",
    "\n",
    "classifier.show_most_informative_features(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result of sample review:  Joy\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sample_review = \"My sickness got worse, and the doctors won't do anything\"\n",
    "\n",
    "#get features specific to the input text\n",
    "sample_features = {word:True for word in sample_review.split()}\n",
    "\n",
    "\n",
    "sample_review_doc_feats = get_document_features(sample_review.split(),sample_features)\n",
    "\n",
    "\n",
    "#print('Sample review features: \\n\\n',sample_review_doc_feats)\n",
    "\n",
    "print('result of sample review: ', classifier.classify(sample_review_doc_feats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loads in Patient Journey labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_patient = pd.read_excel('Diabetes-classification.xlsx', sheet_name='Patient-journey') # Reads in excel\n",
    "\n",
    "# Preparing dataset\n",
    "x_journey = df_patient.loc[:,'discussion_text']\n",
    "y_journey = df_patient.loc[:,'Label']\n",
    "# removes all duplicates from list \n",
    "Labels_journey = list(dict.fromkeys(y_journey)) \n",
    "#stopwords\n",
    "lim_punc_patient = [char for char in string.punctuation if char in \"&#^_\"]\n",
    "nopunc_patient = [char for char in x_journey if char not in lim_punc_patient]\n",
    "nopunc_patient = ''.join(nopunc_patient)\n",
    "\n",
    "ext_stopwords_patient=stopwords.words('english')+other_stop\n",
    "clean_words = [word for word in nopunc_patient.split() if word.lower() not in ext_stopwords_patient]\n",
    "\n",
    "# puts discussion_text to a str and tokenize it\n",
    "raw_text_journey = df_patient['discussion_text'].str.cat()\n",
    "tokens_journey = nltk.word_tokenize(raw_text_journey)\n",
    "tokens_emotion_filtered = [clean_words for clean_words in tokens_journey if clean_words.isalnum()]\n",
    "text_journey = nltk.Text(tokens_journey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Living with diabetes - Nutrition\n",
      "Clinical Treatment\n",
      "+------------------------------------------+----------------------------------+\n",
      "|             Discussion text              |             Emotion              |\n",
      "+------------------------------------------+----------------------------------+\n",
      "| Hi,any,one,any,recipes,suitable,for,diab | Living with diabetes - Nutrition |\n",
      "| etes,2,that,can,be,cooked,in,an,Halogen, |                                  |\n",
      "|        Oven,or,Air,fryer...,Stan         |                                  |\n",
      "|                                          |                                  |\n",
      "|                                          |                                  |\n",
      "| And,the,INSULINX,is,the,REAL,DEVICE!.,Fa |        Clinical Treatment        |\n",
      "| ntastic!.,68,ssooty53,said:,,Must,admit, |                                  |\n",
      "| I,don't,pay,for,mine,-,in,UK,both,the,ma |                                  |\n",
      "| chines,,needles,and,strips,are,free,if,y |                                  |\n",
      "| ou,have,Type,2,Diabetes..,Sorry,,but,not |                                  |\n",
      "| ,correct,at,all!.,Only,free,if,you,are,o |                                  |\n",
      "|                    n                     |                                  |\n",
      "|                                          |                                  |\n",
      "|                                          |                                  |\n",
      "+------------------------------------------+----------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# the reviews will be stored as document pairs of words and category\n",
    "X_list_of_words_journey = [sentence.split(\" \") for sentence in x_journey]\n",
    "documents_journey = list(zip(X_list_of_words_journey, y_journey))\n",
    "\n",
    "#give random order to the documents\n",
    "random.shuffle(documents_journey)\n",
    "\n",
    "tab = PrettyTable(['Discussion text', 'Emotion'])\n",
    "tab.horizontal_char = '-'\n",
    "\n",
    "for (doc, cat) in documents_journey[0:2]:\n",
    "    feats = textwrap.fill(','.join(doc[:50]), width=40)\n",
    "    tab.add_row([ feats, cat])\n",
    "    tab.add_row([ '\\n', '\\n'])\n",
    "    print(cat)\n",
    "\n",
    "print(tab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words from emotion corpus:  116721\n",
      "most freq words:  [('his', 165), ('exercise', 165), ('only', 159), ('she', 159), ('time', 159), ('well', 157), ('any', 157), ('glucose', 156), ('then', 155), ('disease', 154)]\n",
      "word_features[:25]:  ['..', 'i', 'and', 'the', 'to', 'a', 'of', 'diabetes', '2', 'type', 'is', 'in', 'my', 'that', 'with', 'have', 'for', 'it', 'you', 'was', 'on', 'as', 'are', ')', '.']\n"
     ]
    }
   ],
   "source": [
    "print('total words from emotion corpus: ', len(text_journey))\n",
    "\n",
    "# load all the words in freq distribution\n",
    "all_words_journey = nltk.FreqDist(w.lower() for w in text_journey)\n",
    "\n",
    "#construct a list of the 2000 most frequent words in the overall corpus (you can try with other numbers as well)\n",
    "most_freq_words_journey = all_words_journey.most_common(6000)\n",
    "print('most freq words: ', most_freq_words_journey[100:110])\n",
    "\n",
    "word_features_journey = [word for (word, count) in most_freq_words_journey]\n",
    "print('word_features[:25]: ', word_features_journey[:25])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformed document features, printing the first 25 features \n",
      "\n",
      " {'contains(..)': True, 'contains(i)': True, 'contains(and)': True, 'contains(the)': True, 'contains(to)': True, 'contains(a)': True, 'contains(of)': True, 'contains(diabetes)': True, 'contains(2)': True, 'contains(type)': True, 'contains(is)': True, 'contains(in)': True, 'contains(my)': True, 'contains(that)': True, 'contains(with)': True, 'contains(have)': True, 'contains(for)': True, 'contains(it)': True, 'contains(you)': True, 'contains(was)': True, 'contains(on)': True, 'contains(as)': True, 'contains(are)': True, 'contains())': True, 'contains(.)': True}\n"
     ]
    }
   ],
   "source": [
    "def get_document_features_journey(documents_journey, doc_features):\n",
    "    \"\"\"\n",
    "        This function will convert given document into a feature set.\n",
    "        Note that we need to add the feature set that is relevant to the document we are inputting\n",
    "        \n",
    "    \"\"\"\n",
    "    #checking whether a word occurs in a set is much faster than checking whether it occurs in a list \n",
    "    document_words = set(documents_journey)\n",
    "    features = {}\n",
    "    \n",
    "    #the feaures dict will consist of words as keys and boolean value of whether they exist in the document\n",
    "    for word in doc_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "words_doc = text_journey\n",
    "\n",
    "feat_dict = get_document_features(words_doc, word_features_journey)\n",
    "\n",
    "feat_dict_25 = {k: feat_dict[k] for k in list(feat_dict.keys())[:25]}\n",
    "print('transformed document features, printing the first 25 features \\n\\n', feat_dict_25)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47\n",
      "Most Informative Features\n",
      "      contains(exercise) = True           Living : Living =     45.5 : 1.0\n",
      "    contains(colleagues) = True           Altern : Living =     35.5 : 1.0\n",
      "        contains(called) = True           Altern : Living =     32.6 : 1.0\n",
      "       contains(studies) = True           Altern : Living =     29.6 : 1.0\n",
      "    contains(treatments) = True           Altern : Living =     29.6 : 1.0\n",
      "        contains(accept) = True           Altern : Living =     25.4 : 1.0\n",
      "        contains(acidic) = True           Altern : Living =     25.4 : 1.0\n",
      "    contains(additional) = True           Altern : Living =     25.4 : 1.0\n",
      "         contains(adobe) = True           Altern : Living =     25.4 : 1.0\n",
      "        contains(agents) = True           Altern : Living =     25.4 : 1.0\n",
      "      contains(approved) = True           Altern : Living =     25.4 : 1.0\n",
      "        contains(bitter) = True           Altern : Living =     25.4 : 1.0\n",
      "        contains(bought) = True           Altern : Living =     25.4 : 1.0\n",
      "    contains(conclusion) = True           Altern : Living =     25.4 : 1.0\n",
      "      contains(download) = True           Altern : Living =     25.4 : 1.0\n",
      "       contains(embrace) = True           Altern : Living =     25.4 : 1.0\n",
      "       contains(failing) = True           Altern : Living =     25.4 : 1.0\n",
      "          contains(file) = True           Altern : Living =     25.4 : 1.0\n",
      "    contains(flavonoids) = True           Altern : Living =     25.4 : 1.0\n",
      "        contains(ginger) = True           Altern : Living =     25.4 : 1.0\n"
     ]
    }
   ],
   "source": [
    "#obtain feature sets for all movie reviews\n",
    "featuresets_journey = [(get_document_features_journey(d,word_features_journey), c) for (d,c) in documents_journey]\n",
    "\n",
    "#split into train and test set (you can experiment with distribution here) 100 - 100 og\n",
    "train_set_journey, test_set_journey = featuresets_journey[200:], featuresets_journey[:100]\n",
    "\n",
    "#instantiate classifier\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set_journey)\n",
    "\n",
    "#print accuracy and most informative features\n",
    "print(nltk.classify.accuracy(classifier, test_set_journey)) \n",
    "\n",
    "classifier.show_most_informative_features(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result of sample review:  Living with diabetes - Exercise\n"
     ]
    }
   ],
   "source": [
    "sample_review = \"My doctor told me to start running and go on a diet\"\n",
    "\n",
    "#get features specific to the input text\n",
    "sample_features = {word:True for word in sample_review.split()}\n",
    "\n",
    "\n",
    "sample_review_doc_feats = get_document_features_journey(sample_review.split(),sample_features)\n",
    "\n",
    "\n",
    "#print('Sample review features: \\n\\n',sample_review_doc_feats)\n",
    "\n",
    "print('result of sample review: ', classifier.classify(sample_review_doc_feats))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a8f61be024eba58adef938c9aa1e29e02cb3dece83a5348b1a2dafd16a070453"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
